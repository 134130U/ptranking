
## About ltr_adhoc
By **ltr_adhoc**, we refer to the traditional learning-to-rank methods based on the Empirical Risk Minimization Framework, which is detailed in [ptranking_empirical_risk_minimization.ipynb](https://github.com/ptranking/ptranking.github.io/raw/master/tutorial/).

Major learning-to-rank approaches can be classified into three categories: **pointwise**, **pairwise**, and **listwise**. The key distinctions are the underlying hypotheses, loss functions, the input and output spaces. The typical pointwise approaches include regression-based [], classification-based [], and ordinal regression-based algorithms []. The loss functions of these algorithms is defined on the basis of each individual document. The pairwise approaches care about the relative order between two documents. The goal of learning is to maximize the number of correctly ordered document pairs. The assumption is that the optimal ranking of documents can be achieved if all the document pairs are correctly ordered. Towards this end, many representative methods have been proposed []. The listwise approaches take all the documents associated with the same query in the training data as the input. In particular, there are two types of loss functions when performing listwise learning. For the first type, the loss function is related to a specific evaluation metric (e.g., nDCG and ERR). Due to the non-differentiability and non-decomposability of the commonly used metrics, the methods of this type either try to optimize the upper bounds as surrogate objective functions [] or approximate the target metric using some smooth
functions []. However, there are still some open issues regarding the first type methods. On one hand, some adopted surrogate functions or approximated metrics are not convex, which makes it hard to optimize. On the other hand, the relationship between the surrogate function and the adopted metric has not been sufficiently investigated, which makes it unclear whether optimizing the surrogate functions can indeed optimize the target metric. For the second type, the loss function is not explicitly related to a specific evaluation metric. The loss function reflects the discrepancy between the predicted ranking and the ground-truth ranking. Example algorithms include []. Although no particular evaluation metrics are directly involved and optimized
here, it is possible that the learned ranking function can achieve good performance in terms of evaluation metrics. Our work belongs to the second type. Different from prior studies, we investigate the problem of listwise document ranking from a unique perspective of optimal transport. The query-level ranking loss is quantified based on the smoothed Wasserstein distance between the predicted ranking and the ranking derived from ground truth labels, where ranking-specific cost matrix is developed.

